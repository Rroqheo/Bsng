#!/usr/bin/env python3
"""
é‡åŒ–å¤§æ¨¡å‹ä»¥é€‚é…M2 Ultra 64GBå†…å­˜
æ”¯æŒï¼šLlama-2-70B, CodeLlama-34B, Yi-34B, Qwen-72B
"""

import torch
import os
from transformers import AutoTokenizer, AutoModelForCausalLM
from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig
import gc

class LargeModelQuantizer:
    def __init__(self):
        self.device = "mps" if torch.backends.mps.is_available() else "cpu"
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
    def quantize_model(self, model_name, output_dir, bits=4):
        """é‡åŒ–å¤§æ¨¡å‹"""
        print(f"å¼€å§‹é‡åŒ–æ¨¡å‹: {model_name}")
        print(f"é‡åŒ–ä½æ•°: {bits}-bit")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # åŠ è½½tokenizer
        print("åŠ è½½tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        # é‡åŒ–é…ç½®
        quantize_config = BaseQuantizeConfig(
            bits=bits,
            group_size=128,
            desc_act=False
        )
        
        # é‡åŒ–æ¨¡å‹
        print("å¼€å§‹é‡åŒ–...")
        model = AutoGPTQForCausalLM.from_pretrained(
            model_name,
            quantize_config=quantize_config,
            device_map="auto",
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True
        )
        
        # ä¿å­˜é‡åŒ–æ¨¡å‹
        print("ä¿å­˜é‡åŒ–æ¨¡å‹...")
        model.save_quantized(output_dir)
        tokenizer.save_pretrained(output_dir)
        
        # æ¸…ç†å†…å­˜
        del model
        gc.collect()
        torch.mps.empty_cache()
        
        print(f"âœ… é‡åŒ–å®Œæˆï¼ä¿å­˜åˆ°: {output_dir}")

def main():
    print("=== ğŸš€ M2 Ultra å¤§æ¨¡å‹é‡åŒ–å·¥å…· ===")
    print("å°†å¤§æ¨¡å‹å‹ç¼©åˆ°64GBå†…å­˜å†…")
    
    quantizer = LargeModelQuantizer()
    
    # ç¤ºä¾‹ï¼šé‡åŒ–CodeLlama-34B
    quantizer.quantize_model(
        "codellama/CodeLlama-34b-hf",
        "./models/CodeLlama-34B-4bit",
        4
    )

if __name__ == "__main__":
    main()
