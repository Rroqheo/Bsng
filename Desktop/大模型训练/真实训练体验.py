#!/usr/bin/env python3
"""
çœŸå®çš„ä¸­æ–‡æ¨¡å‹è®­ç»ƒä½“éªŒ
ä½¿ç”¨çœŸå®çš„ä¸­æ–‡æ•°æ®å’Œå®Œæ•´çš„è®­ç»ƒè¿‡ç¨‹
"""

import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from datasets import Dataset
import time

class RealChineseTrainer:
    def __init__(self):
        self.device = "mps" if torch.backends.mps.is_available() else "cpu"
        print(f"=== ğŸ¯ çœŸå®ä¸­æ–‡æ¨¡å‹è®­ç»ƒä½“éªŒ ===")
        print(f"è®¾å¤‡: {self.device}")
        print(f"å†…å­˜: {torch.mps.current_allocated_memory()/1024**3:.1f}GB / 64GB")
        
    def load_real_model(self, model_name="microsoft/DialoGPT-medium"):
        """åŠ è½½çœŸå®çš„é¢„è®­ç»ƒæ¨¡å‹"""
        print(f"ğŸ”„ åŠ è½½çœŸå®æ¨¡å‹: {model_name}")
        print("æ­£åœ¨ä¸‹è½½æ¨¡å‹æ–‡ä»¶...")
        
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            low_cpu_mem_usage=True
        )
        
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼å‚æ•°æ•°é‡: {model.num_parameters():,}")
        return model, tokenizer
    
    def create_real_chinese_dataset(self, tokenizer):
        """åˆ›å»ºçœŸå®çš„ä¸­æ–‡è®­ç»ƒæ•°æ®"""
        print("ğŸ“š å‡†å¤‡çœŸå®ä¸­æ–‡è®­ç»ƒæ•°æ®...")
        
        # çœŸå®çš„ä¸­æ–‡AIç›¸å…³æ–‡æœ¬æ•°æ®
        chinese_texts = [
            "äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼ŒAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚",
            "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é›†ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ å’Œæ”¹è¿›ã€‚é€šè¿‡åˆ†æå¤§é‡æ•°æ®ï¼Œæœºå™¨å­¦ä¹ ç®—æ³•å¯ä»¥å‘ç°æ¨¡å¼å¹¶åšå‡ºé¢„æµ‹ã€‚",
            "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼ã€‚å®ƒåœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œè¯­éŸ³è¯†åˆ«ç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚",
            "è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé¢†åŸŸï¼Œä¸“æ³¨äºè®¡ç®—æœºç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€çš„èƒ½åŠ›ã€‚å®ƒåŒ…æ‹¬æœºå™¨ç¿»è¯‘ã€æƒ…æ„Ÿåˆ†æã€é—®ç­”ç³»ç»Ÿç­‰åº”ç”¨ã€‚",
            "è®¡ç®—æœºè§†è§‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°å­—å›¾åƒæˆ–è§†é¢‘ä¸­è·å–é«˜å±‚æ¬¡ç†è§£ã€‚å®ƒåœ¨è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—è¯Šæ–­å’Œå®‰å…¨ç›‘æ§ç­‰é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ã€‚",
            "å¼ºåŒ–å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ç§æ–¹æ³•ï¼Œå…¶ä¸­ä»£ç†é€šè¿‡ä¸ç¯å¢ƒäº¤äº’æ¥å­¦ä¹ æœ€ä¼˜è¡Œä¸ºç­–ç•¥ã€‚å®ƒåœ¨æ¸¸æˆAIã€æœºå™¨äººæ§åˆ¶å’Œæ¨èç³»ç»Ÿç­‰é¢†åŸŸè¡¨ç°å‡ºè‰²ã€‚",
            "ç¥ç»ç½‘ç»œæ˜¯å—äººè„‘å¯å‘çš„è®¡ç®—æ¨¡å‹ï¼Œç”±ç›¸äº’è¿æ¥çš„èŠ‚ç‚¹å±‚ç»„æˆã€‚æ¯ä¸ªèŠ‚ç‚¹æ¥æ”¶è¾“å…¥ï¼Œåº”ç”¨æ¿€æ´»å‡½æ•°ï¼Œå¹¶äº§ç”Ÿè¾“å‡ºä¼ é€’ç»™ä¸‹ä¸€å±‚ã€‚",
            "å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ˜¯ä¸€ç§ä¸“é—¨ç”¨äºå¤„ç†ç½‘æ ¼ç»“æ„æ•°æ®çš„ç¥ç»ç½‘ç»œï¼Œå¦‚å›¾åƒã€‚å®ƒé€šè¿‡å·ç§¯å±‚æå–ç‰¹å¾ï¼Œåœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚",
            "å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰æ˜¯ä¸€ç§ç¥ç»ç½‘ç»œï¼Œè®¾è®¡ç”¨äºå¤„ç†åºåˆ—æ•°æ®ï¼Œå¦‚æ–‡æœ¬æˆ–æ—¶é—´åºåˆ—ã€‚å®ƒèƒ½å¤Ÿè®°ä½ä¹‹å‰çš„ä¿¡æ¯ï¼Œé€‚åˆå¤„ç†æœ‰é¡ºåºçš„æ•°æ®ã€‚",
            "Transformeræ˜¯ä¸€ç§ç¥ç»ç½‘ç»œæ¶æ„ï¼Œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚å®ƒä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶æ¥å…³æ³¨è¾“å…¥çš„ä¸åŒéƒ¨åˆ†ï¼Œåœ¨æœºå™¨ç¿»è¯‘å’Œæ–‡æœ¬ç”Ÿæˆæ–¹é¢å–å¾—äº†é©å‘½æ€§è¿›å±•ã€‚",
            "æ³¨æ„åŠ›æœºåˆ¶æ˜¯ç¥ç»ç½‘ç»œä¸­çš„ä¸€ç§æŠ€æœ¯ï¼Œå…è®¸æ¨¡å‹å…³æ³¨è¾“å…¥çš„ä¸åŒéƒ¨åˆ†ã€‚å®ƒä½¿æ¨¡å‹èƒ½å¤Ÿç†è§£ä¸Šä¸‹æ–‡å…³ç³»ï¼Œåœ¨é•¿æ–‡æœ¬å¤„ç†ä¸­ç‰¹åˆ«æœ‰æ•ˆã€‚",
            "è¿ç§»å­¦ä¹ æ˜¯ä¸€ç§æœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œå°†åœ¨ä¸€ä¸ªä»»åŠ¡ä¸Šå­¦åˆ°çš„çŸ¥è¯†åº”ç”¨åˆ°ç›¸å…³ä»»åŠ¡ä¸Šã€‚å®ƒèƒ½å¤Ÿå‡å°‘è®­ç»ƒæ—¶é—´å’Œæ•°æ®éœ€æ±‚ï¼Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚",
            "æ•°æ®å¢å¼ºæ˜¯ä¸€ç§æŠ€æœ¯ï¼Œé€šè¿‡åˆ›å»ºç°æœ‰æ•°æ®çš„ä¿®æ”¹ç‰ˆæœ¬æ¥å¢åŠ è®­ç»ƒæ•°æ®é‡ã€‚å®ƒåŒ…æ‹¬æ—‹è½¬ã€ç¼©æ”¾ã€å™ªå£°æ·»åŠ ç­‰æ–¹æ³•ï¼Œèƒ½å¤Ÿæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚",
            "è¿‡æ‹Ÿåˆæ˜¯æœºå™¨å­¦ä¹ ä¸­çš„ä¸€ä¸ªé—®é¢˜ï¼Œæ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æ–°æ•°æ®ä¸Šè¡¨ç°ä¸ä½³ã€‚å®ƒé€šå¸¸æ˜¯ç”±äºæ¨¡å‹è¿‡äºå¤æ‚æˆ–è®­ç»ƒæ•°æ®ä¸è¶³å¯¼è‡´çš„ã€‚",
            "æ­£åˆ™åŒ–æ˜¯é˜²æ­¢è¿‡æ‹Ÿåˆçš„æŠ€æœ¯ï¼Œé€šè¿‡æ·»åŠ çº¦æŸæˆ–æƒ©ç½šé¡¹åˆ°æ¨¡å‹ã€‚å®ƒåŒ…æ‹¬L1æ­£åˆ™åŒ–ã€L2æ­£åˆ™åŒ–ã€Dropoutç­‰æ–¹æ³•ï¼Œèƒ½å¤Ÿæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚",
            "ä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯ï¼Œä¸“æ³¨äºç†è§£å’Œç”Ÿæˆä¸­æ–‡æ–‡æœ¬ã€‚å®ƒé¢ä¸´åˆ†è¯ã€è¯­ä¹‰ç†è§£ã€æ­§ä¹‰æ¶ˆè§£ç­‰ç‹¬ç‰¹æŒ‘æˆ˜ã€‚",
            "ä¸­æ–‡åˆ†è¯æ˜¯ä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†çš„åŸºç¡€æŠ€æœ¯ï¼Œå°†è¿ç»­çš„ä¸­æ–‡å­—ç¬¦åºåˆ—åˆ‡åˆ†æˆæœ‰æ„ä¹‰çš„è¯æ±‡å•å…ƒã€‚å®ƒæ˜¯ä¸­æ–‡æ–‡æœ¬å¤„ç†çš„ç¬¬ä¸€æ­¥ï¼Œå¯¹åç»­ä»»åŠ¡è‡³å…³é‡è¦ã€‚",
            "ä¸­æ–‡è¯­ä¹‰ç†è§£æ˜¯è®©è®¡ç®—æœºç†è§£ä¸­æ–‡æ–‡æœ¬å«ä¹‰çš„æŠ€æœ¯ï¼ŒåŒ…æ‹¬è¯ä¹‰æ¶ˆæ­§ã€è¯­ä¹‰è§’è‰²æ ‡æ³¨ã€æŒ‡ä»£æ¶ˆè§£ç­‰ã€‚å®ƒéœ€è¦æ·±å…¥ç†è§£ä¸­æ–‡çš„è¯­è¨€ç‰¹ç‚¹å’Œè¡¨è¾¾æ–¹å¼ã€‚",
            "ä¸­æ–‡æœºå™¨ç¿»è¯‘æ˜¯å°†ä¸­æ–‡æ–‡æœ¬ç¿»è¯‘æˆå…¶ä»–è¯­è¨€æˆ–å°†å…¶ä»–è¯­è¨€ç¿»è¯‘æˆä¸­æ–‡çš„æŠ€æœ¯ã€‚å®ƒéœ€è¦è€ƒè™‘ä¸­æ–‡çš„è¯­æ³•ç»“æ„ã€æ–‡åŒ–èƒŒæ™¯å’Œè¡¨è¾¾ä¹ æƒ¯ã€‚",
            "ä¸­æ–‡æƒ…æ„Ÿåˆ†ææ˜¯åˆ†æä¸­æ–‡æ–‡æœ¬æƒ…æ„Ÿå€¾å‘çš„æŠ€æœ¯ï¼Œå¹¿æ³›åº”ç”¨äºç¤¾äº¤åª’ä½“ã€äº§å“è¯„è®ºã€èˆ†æƒ…ç›‘æµ‹ç­‰é¢†åŸŸã€‚å®ƒéœ€è¦ç†è§£ä¸­æ–‡çš„æƒ…æ„Ÿè¡¨è¾¾æ–¹å¼å’Œè¯­å¢ƒã€‚"
        ]
        
        print(f"ğŸ“– å‡†å¤‡ {len(chinese_texts)} æ¡é«˜è´¨é‡ä¸­æ–‡è®­ç»ƒæ•°æ®...")
        
        # çœŸå®çš„tokenization
        tokenized_data = []
        for i, text in enumerate(chinese_texts):
            print(f"å¤„ç†æ•°æ® {i+1}/{len(chinese_texts)}: {text[:30]}...")
            
            # ä½¿ç”¨tokenizerè¿›è¡Œç¼–ç 
            encoded = tokenizer(
                text,
                truncation=True,
                padding="max_length",
                max_length=256,
                return_tensors="pt"
            )
            
            tokenized_data.append({
                "input_ids": encoded["input_ids"][0].tolist(),
                "attention_mask": encoded["attention_mask"][0].tolist()
            })
            
            # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            time.sleep(0.1)
        
        print(f"âœ… æ•°æ®å¤„ç†å®Œæˆï¼å…± {len(tokenized_data)} æ¡è®­ç»ƒæ ·æœ¬")
        return Dataset.from_list(tokenized_data)
    
    def create_real_training_args(self, output_dir="./real_chinese_results"):
        """åˆ›å»ºçœŸå®çš„è®­ç»ƒå‚æ•°"""
        print("âš™ï¸ é…ç½®çœŸå®è®­ç»ƒå‚æ•°...")
        
        return TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=3,  # çœŸå®è®­ç»ƒè½®æ•°
            per_device_train_batch_size=2,
            per_device_eval_batch_size=2,
            gradient_accumulation_steps=8,
            warmup_steps=100,
            weight_decay=0.01,
            logging_steps=5,
            save_steps=50,
            eval_steps=50,
            save_strategy="steps",
            eval_strategy="steps",
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            fp16=False,  # ç¦ç”¨fp16é¿å…MPSé—®é¢˜
            dataloader_pin_memory=False,
            remove_unused_columns=False,
            report_to=None,
            dataloader_num_workers=0,
            max_grad_norm=1.0,
            learning_rate=3e-5,  # çœŸå®å­¦ä¹ ç‡
            lr_scheduler_type="cosine",
            gradient_checkpointing=True,
            optim="adamw_torch",
        )
    
    def train_real_model(self, model_path="microsoft/DialoGPT-medium", output_dir="./real_chinese_results"):
        """å¼€å§‹çœŸå®è®­ç»ƒ"""
        print("ğŸš€ å¼€å§‹çœŸå®ä¸­æ–‡æ¨¡å‹è®­ç»ƒä½“éªŒ...")
        print("=" * 50)
        
        # åŠ è½½æ¨¡å‹å’Œtokenizer
        model, tokenizer = self.load_real_model(model_path)
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = self.create_real_chinese_dataset(tokenizer)
        eval_dataset = self.create_real_chinese_dataset(tokenizer)
        
        # è®­ç»ƒå‚æ•°
        training_args = self.create_real_training_args(output_dir)
        
        # æ•°æ®æ•´ç†å™¨
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=tokenizer,
            mlm=False,
        )
        
        # è®­ç»ƒå™¨
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            data_collator=data_collator,
        )
        
        # å¼€å§‹çœŸå®è®­ç»ƒ
        print("ğŸ”¥ å¼€å§‹çœŸå®è®­ç»ƒè¿‡ç¨‹...")
        print("é¢„è®¡è®­ç»ƒæ—¶é—´: 5-10åˆ†é’Ÿ")
        print("è®­ç»ƒè¿›åº¦:")
        
        start_time = time.time()
        trainer.train()
        end_time = time.time()
        
        # ä¿å­˜æ¨¡å‹
        print("ğŸ’¾ ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹...")
        trainer.save_model()
        
        training_time = end_time - start_time
        print(f"âœ… çœŸå®è®­ç»ƒå®Œæˆï¼")
        print(f"â±ï¸ è®­ç»ƒæ—¶é—´: {training_time/60:.1f}åˆ†é’Ÿ")
        print(f"ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: {output_dir}")
        
        return trainer

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ æ¬¢è¿ä½“éªŒçœŸå®çš„ä¸­æ–‡æ¨¡å‹è®­ç»ƒï¼")
    print("è¿™æ¬¡æˆ‘ä»¬å°†ä½¿ç”¨çœŸå®çš„ä¸­æ–‡æ•°æ®å’Œå®Œæ•´çš„è®­ç»ƒè¿‡ç¨‹")
    print("=" * 50)
    
    trainer = RealChineseTrainer()
    trainer.train_real_model()

if __name__ == "__main__":
    main()
