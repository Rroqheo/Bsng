#!/usr/bin/env python3
"""
æµ‹è¯•ç®€åŒ–è®­ç»ƒçš„ä¸­æ–‡æ¨¡å‹
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

class SimpleModelTester:
    def __init__(self, model_path="./simple_chinese_results"):
        self.model_path = model_path
        self.device = "mps" if torch.backends.mps.is_available() else "cpu"
        print(f"=== ğŸ§ª æµ‹è¯•ç®€åŒ–è®­ç»ƒçš„ä¸­æ–‡æ¨¡å‹ ===")
        print(f"æ¨¡å‹è·¯å¾„: {model_path}")
        print(f"è®¾å¤‡: {self.device}")
        
    def load_model(self):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
        
        # åŠ è½½tokenizer
        tokenizer = AutoTokenizer.from_pretrained(self.model_path)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            
        # åŠ è½½æ¨¡å‹
        model = AutoModelForCausalLM.from_pretrained(
            self.model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            low_cpu_mem_usage=True
        )
        
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        return model, tokenizer
    
    def test_chinese_dialogue(self, model, tokenizer):
        """æµ‹è¯•ä¸­æ–‡å¯¹è¯"""
        print("\n=== ğŸ—£ï¸ ä¸­æ–‡å¯¹è¯æµ‹è¯• ===")
        
        test_questions = [
            "äººå·¥æ™ºèƒ½æ˜¯ä»€ä¹ˆï¼Ÿ",
            "æœºå™¨å­¦ä¹ çš„åŸºæœ¬åŸç†æ˜¯ä»€ä¹ˆï¼Ÿ",
            "æ·±åº¦å­¦ä¹ ä¸ä¼ ç»Ÿæœºå™¨å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
            "è‡ªç„¶è¯­è¨€å¤„ç†æœ‰å“ªäº›åº”ç”¨ï¼Ÿ",
            "ä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†æœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ"
        ]
        
        for question in test_questions:
            print(f"\né—®é¢˜: {question}")
            
            # ç¼–ç è¾“å…¥
            inputs = tokenizer(question, return_tensors="pt", padding=True)
            
            # å°†è¾“å…¥ç§»åˆ°æ­£ç¡®çš„è®¾å¤‡
            if self.device == "mps":
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # ç”Ÿæˆå›ç­”
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_length=100,
                    num_return_sequences=1,
                    temperature=1.0,
                    do_sample=False,  # ä½¿ç”¨è´ªå©ªè§£ç 
                    pad_token_id=tokenizer.eos_token_id,
                    eos_token_id=tokenizer.eos_token_id
                )
            
            # è§£ç è¾“å‡º
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            print(f"å›ç­”: {response}")
            print("-" * 50)
    
    def test_text_generation(self, model, tokenizer):
        """æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ"""
        print("\n=== ğŸ“ æ–‡æœ¬ç”Ÿæˆæµ‹è¯• ===")
        
        prompts = [
            "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œ",
            "äººå·¥æ™ºèƒ½æŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œ",
            "ä¸­æ–‡æ˜¯ä¸–ç•Œä¸Šæœ€å¤è€çš„è¯­è¨€ä¹‹ä¸€ï¼Œ"
        ]
        
        for prompt in prompts:
            print(f"\næç¤º: {prompt}")
            
            # ç¼–ç è¾“å…¥
            inputs = tokenizer(prompt, return_tensors="pt", padding=True)
            
            # å°†è¾“å…¥ç§»åˆ°æ­£ç¡®çš„è®¾å¤‡
            if self.device == "mps":
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # ç”Ÿæˆæ–‡æœ¬
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_length=80,
                    num_return_sequences=1,
                    temperature=1.0,
                    do_sample=False,  # ä½¿ç”¨è´ªå©ªè§£ç 
                    pad_token_id=tokenizer.eos_token_id,
                    eos_token_id=tokenizer.eos_token_id
                )
            
            # è§£ç è¾“å‡º
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            print(f"ç”Ÿæˆ: {generated_text}")
            print("-" * 50)
    
    def test_model_performance(self, model, tokenizer):
        """æµ‹è¯•æ¨¡å‹æ€§èƒ½"""
        print("\n=== âš¡ æ€§èƒ½æµ‹è¯• ===")
        
        import time
        
        test_text = "äººå·¥æ™ºèƒ½"
        inputs = tokenizer(test_text, return_tensors="pt", padding=True)
        
        # å°†è¾“å…¥ç§»åˆ°æ­£ç¡®çš„è®¾å¤‡
        if self.device == "mps":
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # æµ‹è¯•æ¨ç†é€Ÿåº¦
        start_time = time.time()
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=30,
                num_return_sequences=1,
                do_sample=False,  # ä½¿ç”¨è´ªå©ªè§£ç 
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        end_time = time.time()
        
        inference_time = end_time - start_time
        print(f"æ¨ç†æ—¶é—´: {inference_time:.2f}ç§’")
        
        # æµ‹è¯•å†…å­˜ä½¿ç”¨
        if torch.backends.mps.is_available():
            allocated = torch.mps.current_allocated_memory() / 1024**3
            print(f"æ¨¡å‹å†…å­˜ä½¿ç”¨: {allocated:.2f} GB")

def main():
    """ä¸»å‡½æ•°"""
    tester = SimpleModelTester()
    
    try:
        # åŠ è½½æ¨¡å‹
        model, tokenizer = tester.load_model()
        
        # æµ‹è¯•å¯¹è¯
        tester.test_chinese_dialogue(model, tokenizer)
        
        # æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ
        tester.test_text_generation(model, tokenizer)
        
        # æµ‹è¯•æ€§èƒ½
        tester.test_model_performance(model, tokenizer)
        
        print("\nğŸ‰ æ¨¡å‹æµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        print("è¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”å®Œæ•´")

if __name__ == "__main__":
    main()
