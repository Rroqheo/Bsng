#!/usr/bin/env python3
"""
ç®€åŒ–çš„ä¸­æ–‡æ¨¡å‹è®­ç»ƒè„šæœ¬
é¿å…tokenizerå¹¶è¡ŒåŒ–é—®é¢˜
"""

import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from datasets import Dataset

class SimpleChineseTrainer:
    def __init__(self):
        self.device = "mps" if torch.backends.mps.is_available() else "cpu"
        print(f"=== ğŸš€ ç®€åŒ–ä¸­æ–‡æ¨¡å‹è®­ç»ƒ ===")
        print(f"è®¾å¤‡: {self.device}")
        
    def load_model_and_tokenizer(self, model_name="microsoft/DialoGPT-medium"):
        """åŠ è½½æ¨¡å‹å’Œtokenizer"""
        print(f"åŠ è½½æ¨¡å‹: {model_name}")
        
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            low_cpu_mem_usage=True
        )
        
        return model, tokenizer
    
    def create_simple_dataset(self, tokenizer):
        """åˆ›å»ºç®€å•çš„è®­ç»ƒæ•°æ®é›†"""
        chinese_texts = [
            "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ã€‚",
            "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é›†ã€‚",
            "æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œã€‚",
            "è‡ªç„¶è¯­è¨€å¤„ç†ä¸“æ³¨äºè¯­è¨€ç†è§£ã€‚",
            "è®¡ç®—æœºè§†è§‰å¤„ç†å›¾åƒå’Œè§†é¢‘ã€‚",
            "å¼ºåŒ–å­¦ä¹ é€šè¿‡äº¤äº’å­¦ä¹ ã€‚",
            "ç¥ç»ç½‘ç»œæ¨¡æ‹Ÿäººè„‘ç»“æ„ã€‚",
            "å·ç§¯ç¥ç»ç½‘ç»œå¤„ç†å›¾åƒæ•°æ®ã€‚",
            "å¾ªç¯ç¥ç»ç½‘ç»œå¤„ç†åºåˆ—æ•°æ®ã€‚",
            "Transformeråœ¨NLPä¸­è¡¨ç°å‡ºè‰²ã€‚"
        ]
        
        # ç®€å•çš„tokenization
        tokenized_data = []
        for text in chinese_texts:
            # ä½¿ç”¨tokenizerè¿›è¡Œç¼–ç 
            encoded = tokenizer(
                text,
                truncation=True,
                padding="max_length",
                max_length=128,
                return_tensors="pt"
            )
            
            tokenized_data.append({
                "input_ids": encoded["input_ids"][0].tolist(),
                "attention_mask": encoded["attention_mask"][0].tolist()
            })
        
        return Dataset.from_list(tokenized_data)
    
    def create_training_args(self, output_dir="./simple_chinese_results"):
        """åˆ›å»ºè®­ç»ƒå‚æ•°"""
        return TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=2,
            per_device_train_batch_size=2,
            per_device_eval_batch_size=2,
            gradient_accumulation_steps=4,
            warmup_steps=50,
            weight_decay=0.01,
            logging_steps=10,
            save_steps=100,
            eval_steps=100,
            save_strategy="steps",
            eval_strategy="steps",
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            fp16=False,  # ç¦ç”¨fp16é¿å…MPSé—®é¢˜
            dataloader_pin_memory=False,
            remove_unused_columns=False,
            report_to=None,
            dataloader_num_workers=0,  # ç¦ç”¨å¤šè¿›ç¨‹
            max_grad_norm=1.0,
            learning_rate=5e-5,
            lr_scheduler_type="cosine",
            gradient_checkpointing=True,
            optim="adamw_torch",
        )
    
    def train(self, model_path="microsoft/DialoGPT-medium", output_dir="./simple_chinese_results"):
        """å¼€å§‹è®­ç»ƒ"""
        print("å¼€å§‹ç®€åŒ–ä¸­æ–‡æ¨¡å‹è®­ç»ƒ...")
        
        # åŠ è½½æ¨¡å‹å’Œtokenizer
        model, tokenizer = self.load_model_and_tokenizer(model_path)
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = self.create_simple_dataset(tokenizer)
        eval_dataset = self.create_simple_dataset(tokenizer)
        
        # è®­ç»ƒå‚æ•°
        training_args = self.create_training_args(output_dir)
        
        # æ•°æ®æ•´ç†å™¨
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=tokenizer,
            mlm=False,
        )
        
        # è®­ç»ƒå™¨
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            data_collator=data_collator,
        )
        
        # å¼€å§‹è®­ç»ƒ
        print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
        trainer.train()
        
        # ä¿å­˜æ¨¡å‹
        trainer.save_model()
        print(f"âœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜åœ¨: {output_dir}")
        
        return trainer

def main():
    """ä¸»å‡½æ•°"""
    trainer = SimpleChineseTrainer()
    trainer.train()

if __name__ == "__main__":
    main()
