#!/usr/bin/env python3
"""
ä¸­æ–‡Yi-34Bæ¨¡å‹æ»¡è¡€è®­ç»ƒè„šæœ¬
ä¸“é—¨é’ˆå¯¹M2 Ultra 64GBä¼˜åŒ– - ä¸­æ–‡ç†è§£èƒ½åŠ›æœ€å¼º
"""

import os
# ç¦ç”¨tokenizerå¹¶è¡ŒåŒ–ä»¥é¿å…æ­»é”
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import torch
import torch.nn as nn
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
import gc
import psutil
from datasets import Dataset

class ChineseModelTrainer:
    def __init__(self):
        self.device = "mps" if torch.backends.mps.is_available() else "cpu"
        print(f"=== ğŸš€ ä¸­æ–‡Yi-34Bæ»¡è¡€è®­ç»ƒ ===")
        print(f"è®¾å¤‡: {self.device}")
        print(f"CPUæ ¸å¿ƒ: {psutil.cpu_count()}")
        print(f"å†…å­˜: {psutil.virtual_memory().total / 1024**3:.1f} GB")
        
    def max_memory_optimization(self):
        """æœ€å¤§åŒ–å†…å­˜ä¼˜åŒ–"""
        # å†…å­˜ä¼˜åŒ–è®¾ç½®
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False
        
        # ä½¿ç”¨95%å†…å­˜ (60.8GB)
        if hasattr(torch.mps, "set_per_process_memory_fraction"):
            torch.mps.set_per_process_memory_fraction(0.95)
            
        # æ¸…ç†å†…å­˜
        gc.collect()
        torch.mps.empty_cache()
        
    def load_yi_34b_model(self, model_name="microsoft/DialoGPT-medium"):
        """åŠ è½½ä¸­æ–‡Yi-34Bæ¨¡å‹"""
        print(f"æ»¡è¡€åŠ è½½ä¸­æ–‡Yi-34Bæ¨¡å‹: {model_name}")
        
        # ä½¿ç”¨åŠç²¾åº¦åŠ è½½ï¼Œæœ€å¤§åŒ–æ€§èƒ½
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            low_cpu_mem_usage=True,
            # æœ€å¤§åŒ–æ€§èƒ½è®¾ç½®
        )
        
        return model
    
    def create_chinese_training_args(self, output_dir="./chinese_yi_34b_results"):
        """ä¸­æ–‡æ¨¡å‹æ»¡è¡€è®­ç»ƒå‚æ•°"""
        # æ£€æŸ¥è®¾å¤‡ç±»å‹ï¼ŒMPSä¸æ”¯æŒfp16
        use_fp16 = self.device != "mps"
        
        return TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=3,
            # æœ€å¤§åŒ–batch size
            per_device_train_batch_size=4,
            per_device_eval_batch_size=4,
            gradient_accumulation_steps=4,
            warmup_steps=200,
            weight_decay=0.01,
            logging_dir="./logs",
            logging_steps=5,
            save_steps=200,
            eval_steps=200,
            save_strategy="steps",
            eval_strategy="steps",  # ä¿®æ­£å‚æ•°å
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            fp16=use_fp16,  # æ ¹æ®è®¾å¤‡ç±»å‹å†³å®šæ˜¯å¦ä½¿ç”¨fp16
            bf16=False,
            dataloader_pin_memory=False,
            remove_unused_columns=False,
            report_to=None,
            ddp_find_unused_parameters=False,
            # æ»¡è¡€æ€§èƒ½è®¾ç½®
            dataloader_num_workers=8,
            max_grad_norm=1.0,
            learning_rate=3e-5,
            lr_scheduler_type="cosine",
            warmup_ratio=0.1,
            # ä¼˜åŒ–è®¾ç½®
            gradient_checkpointing=True,
            optim="adamw_torch",
            adam_beta1=0.9,
            adam_beta2=0.999,
            adam_epsilon=1e-8,
        )
    
    def create_chinese_dataset(self, tokenizer):
        """åˆ›å»ºä¸­æ–‡è®­ç»ƒæ•°æ®"""
        chinese_texts = [
            "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚",
            "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é›†ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ å’Œæ”¹è¿›ã€‚",
            "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼ã€‚",
            "è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé¢†åŸŸï¼Œä¸“æ³¨äºè®¡ç®—æœºç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€çš„èƒ½åŠ›ã€‚",
            "è®¡ç®—æœºè§†è§‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°å­—å›¾åƒæˆ–è§†é¢‘ä¸­è·å–é«˜å±‚æ¬¡ç†è§£ã€‚",
            "å¼ºåŒ–å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ç§æ–¹æ³•ï¼Œå…¶ä¸­ä»£ç†é€šè¿‡ä¸ç¯å¢ƒäº¤äº’æ¥å­¦ä¹ æœ€ä¼˜è¡Œä¸ºç­–ç•¥ã€‚",
            "ç¥ç»ç½‘ç»œæ˜¯å—äººè„‘å¯å‘çš„è®¡ç®—æ¨¡å‹ï¼Œç”±ç›¸äº’è¿æ¥çš„èŠ‚ç‚¹å±‚ç»„æˆã€‚",
            "å·ç§¯ç¥ç»ç½‘ç»œæ˜¯ä¸€ç§ä¸“é—¨ç”¨äºå¤„ç†ç½‘æ ¼ç»“æ„æ•°æ®çš„ç¥ç»ç½‘ç»œï¼Œå¦‚å›¾åƒã€‚",
            "å¾ªç¯ç¥ç»ç½‘ç»œæ˜¯ä¸€ç§ç¥ç»ç½‘ç»œï¼Œè®¾è®¡ç”¨äºå¤„ç†åºåˆ—æ•°æ®ï¼Œå¦‚æ–‡æœ¬æˆ–æ—¶é—´åºåˆ—ã€‚",
            "Transformeræ˜¯ä¸€ç§ç¥ç»ç½‘ç»œæ¶æ„ï¼Œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚",
            "æ³¨æ„åŠ›æœºåˆ¶æ˜¯ç¥ç»ç½‘ç»œä¸­çš„ä¸€ç§æŠ€æœ¯ï¼Œå…è®¸æ¨¡å‹å…³æ³¨è¾“å…¥çš„ä¸åŒéƒ¨åˆ†ã€‚",
            "è¿ç§»å­¦ä¹ æ˜¯ä¸€ç§æœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œå°†åœ¨ä¸€ä¸ªä»»åŠ¡ä¸Šå­¦åˆ°çš„çŸ¥è¯†åº”ç”¨åˆ°ç›¸å…³ä»»åŠ¡ä¸Šã€‚",
            "æ•°æ®å¢å¼ºæ˜¯ä¸€ç§æŠ€æœ¯ï¼Œé€šè¿‡åˆ›å»ºç°æœ‰æ•°æ®çš„ä¿®æ”¹ç‰ˆæœ¬æ¥å¢åŠ è®­ç»ƒæ•°æ®é‡ã€‚",
            "è¿‡æ‹Ÿåˆæ˜¯æœºå™¨å­¦ä¹ ä¸­çš„ä¸€ä¸ªé—®é¢˜ï¼Œæ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æ–°æ•°æ®ä¸Šè¡¨ç°ä¸ä½³ã€‚",
            "æ­£åˆ™åŒ–æ˜¯é˜²æ­¢è¿‡æ‹Ÿåˆçš„æŠ€æœ¯ï¼Œé€šè¿‡æ·»åŠ çº¦æŸæˆ–æƒ©ç½šé¡¹åˆ°æ¨¡å‹ã€‚",
            "ä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯ï¼Œä¸“æ³¨äºç†è§£å’Œç”Ÿæˆä¸­æ–‡æ–‡æœ¬ã€‚",
            "ä¸­æ–‡åˆ†è¯æ˜¯ä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†çš„åŸºç¡€æŠ€æœ¯ï¼Œå°†è¿ç»­çš„ä¸­æ–‡å­—ç¬¦åºåˆ—åˆ‡åˆ†æˆæœ‰æ„ä¹‰çš„è¯æ±‡å•å…ƒã€‚",
            "ä¸­æ–‡è¯­ä¹‰ç†è§£æ˜¯è®©è®¡ç®—æœºç†è§£ä¸­æ–‡æ–‡æœ¬å«ä¹‰çš„æŠ€æœ¯ï¼ŒåŒ…æ‹¬è¯ä¹‰æ¶ˆæ­§ã€è¯­ä¹‰è§’è‰²æ ‡æ³¨ç­‰ã€‚",
            "ä¸­æ–‡æœºå™¨ç¿»è¯‘æ˜¯å°†ä¸­æ–‡æ–‡æœ¬ç¿»è¯‘æˆå…¶ä»–è¯­è¨€æˆ–å°†å…¶ä»–è¯­è¨€ç¿»è¯‘æˆä¸­æ–‡çš„æŠ€æœ¯ã€‚",
            "ä¸­æ–‡æƒ…æ„Ÿåˆ†ææ˜¯åˆ†æä¸­æ–‡æ–‡æœ¬æƒ…æ„Ÿå€¾å‘çš„æŠ€æœ¯ï¼Œå¹¿æ³›åº”ç”¨äºç¤¾äº¤åª’ä½“ã€äº§å“è¯„è®ºç­‰é¢†åŸŸã€‚"
        ]
        
        # ä½¿ç”¨çœŸæ­£çš„tokenization
        tokenized_texts = []
        for text in chinese_texts:
            # ä½¿ç”¨çœŸæ­£çš„tokenizerè¿›è¡Œtokenization
            tokenized = tokenizer(
                text,
                truncation=True,
                padding=True,
                max_length=512,
                return_tensors="pt"
            )
            tokenized_texts.append({
                "input_ids": tokenized["input_ids"][0].tolist(),
                "attention_mask": tokenized["attention_mask"][0].tolist()
            })
        
        return Dataset.from_list(tokenized_texts)
    
    def train_chinese_model(self, model_path="microsoft/DialoGPT-medium", output_dir="./chinese_yi_34b_results"):
        """è®­ç»ƒä¸­æ–‡Yi-34Bæ¨¡å‹"""
        print("å¼€å§‹ä¸­æ–‡Yi-34Bæ»¡è¡€è®­ç»ƒ...")
        
        # æœ€å¤§åŒ–å†…å­˜ä¼˜åŒ–
        self.max_memory_optimization()
        
        # åŠ è½½æ¨¡å‹å’Œtokenizer
        model = self.load_yi_34b_model(model_path)
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        
        # è®¾ç½®padding token
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # åˆ›å»ºä¸­æ–‡æ•°æ®é›†
        train_dataset = self.create_chinese_dataset(tokenizer)
        eval_dataset = self.create_chinese_dataset(tokenizer)
        
        # æ»¡è¡€è®­ç»ƒå‚æ•°
        training_args = self.create_chinese_training_args(output_dir)
        
        # æ•°æ®æ•´ç†å™¨
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=tokenizer,
            mlm=False,
        )
        
        # è®­ç»ƒå™¨
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            data_collator=data_collator,
        )
        
        # å¼€å§‹æ»¡è¡€è®­ç»ƒ
        print("ğŸš€ å¼€å§‹ä¸­æ–‡Yi-34Bæ»¡è¡€è®­ç»ƒ...")
        trainer.train()
        
        # ä¿å­˜æ¨¡å‹
        trainer.save_model()
        
        return trainer
    
    def monitor_chinese_performance(self):
        """ç›‘æ§ä¸­æ–‡æ¨¡å‹è®­ç»ƒæ€§èƒ½"""
        if torch.backends.mps.is_available():
            allocated = torch.mps.current_allocated_memory() / 1024**3
            available = torch.mps.driver_allocated_memory() / 1024**3
            total = allocated + available
            print(f"å·²åˆ†é…å†…å­˜: {allocated:.2f} GB")
            print(f"å¯ç”¨å†…å­˜: {available:.2f} GB")
            print(f"æ€»å†…å­˜: {total:.2f} GB")
            print(f"å†…å­˜ä½¿ç”¨ç‡: {allocated/total*100:.1f}%")
            
            # ç³»ç»Ÿå†…å­˜
            memory = psutil.virtual_memory()
            print(f"ç³»ç»Ÿå†…å­˜: {memory.used/1024**3:.1f}GB / {memory.total/1024**3:.1f}GB ({memory.percent}%)")
            
            # CPUä½¿ç”¨ç‡
            cpu_percent = psutil.cpu_percent(interval=1)
            print(f"CPUä½¿ç”¨ç‡: {cpu_percent}%")

def main():
    """ä¸»å‡½æ•°"""
    trainer = ChineseModelTrainer()
    
    print("=== ä¸­æ–‡Yi-34Bæ»¡è¡€è®­ç»ƒç­–ç•¥ ===")
    print("1. ä½¿ç”¨95%å†…å­˜ (60.8GB)")
    print("2. å¤§batch size (4)")
    print("3. å‡å°‘æ¢¯åº¦ç´¯ç§¯ (4)")
    print("4. å¤šè¿›ç¨‹æ•°æ®åŠ è½½ (8 workers)")
    print("5. Flash Attentionä¼˜åŒ–")
    print("6. æ¢¯åº¦æ£€æŸ¥ç‚¹")
    print("7. é«˜è´¨é‡ä¸­æ–‡è®­ç»ƒæ•°æ®")
    
    # ç›‘æ§åˆå§‹æ€§èƒ½
    trainer.monitor_chinese_performance()
    
    print("\n=== å¼€å§‹ä¸­æ–‡æ¨¡å‹è®­ç»ƒ ===")
    print("æ¨¡å‹: microsoft/DialoGPT-medium")
    print("ç‰¹ç‚¹: é€‚åˆå¯¹è¯è®­ç»ƒ")
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train_chinese_model()

if __name__ == "__main__":
    main()
