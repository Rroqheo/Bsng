#!/usr/bin/env python3
"""
å¤§æ¨¡å‹è®­ç»ƒè„šæœ¬ (é€‚é…M2 Ultra 64GBå†…å­˜)
ä½¿ç”¨LoRAå¾®è°ƒæŠ€æœ¯ï¼Œåªè®­ç»ƒå°‘é‡å‚æ•°
"""

import torch
import os
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import Dataset
import gc

class LargeModelTrainer:
    def __init__(self):
        self.device = "mps" if torch.backends.mps.is_available() else "cpu"
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
    def load_quantized_model(self, model_path):
        """åŠ è½½é‡åŒ–å¤§æ¨¡å‹"""
        print(f"åŠ è½½é‡åŒ–æ¨¡å‹: {model_path}")
        
        # ä½¿ç”¨4-bité‡åŒ–åŠ è½½
        from transformers import BitsAndBytesConfig
        
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            quantization_config=bnb_config,
            device_map="auto",
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True
        )
        
        return model
    
    def setup_lora_config(self):
        """LoRAé…ç½® - åªè®­ç»ƒå°‘é‡å‚æ•°"""
        return LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=8,  # é™ä½rankä»¥èŠ‚çœå†…å­˜
            lora_alpha=16,
            lora_dropout=0.1,
            target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
        )
    
    def train_large_model(self, model_path, output_dir="./large_model_results"):
        """è®­ç»ƒå¤§æ¨¡å‹"""
        print("å¼€å§‹è®­ç»ƒå¤§æ¨¡å‹...")
        
        # åŠ è½½æ¨¡å‹å’Œtokenizer
        model = self.load_quantized_model(model_path)
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        
        # è®¾ç½®LoRA
        lora_config = self.setup_lora_config()
        model = get_peft_model(model, lora_config)
        
        # è®­ç»ƒå‚æ•° (é’ˆå¯¹å¤§æ¨¡å‹ä¼˜åŒ–)
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=1,
            per_device_train_batch_size=1,
            gradient_accumulation_steps=32,
            warmup_steps=50,
            weight_decay=0.01,
            logging_steps=5,
            save_steps=100,
            fp16=True,
            dataloader_pin_memory=False,
            remove_unused_columns=False,
            report_to=None,
        )
        
        # å¼€å§‹è®­ç»ƒ
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=Dataset.from_list([{"input_ids": [1,2,3,4,5]}]),
            data_collator=lambda x: {"input_ids": torch.stack([torch.tensor(f) for f in x])}
        )
        
        trainer.train()
        trainer.save_model()
        
        return trainer

def main():
    print("=== ğŸš€ M2 Ultra å¤§æ¨¡å‹è®­ç»ƒå·¥å…· ===")
    print("ä½¿ç”¨LoRAå¾®è°ƒæŠ€æœ¯ï¼Œé€‚é…64GBå†…å­˜")
    
    trainer = LargeModelTrainer()
    
    # é€‰æ‹©æ¨¡å‹è·¯å¾„
    model_path = "./models/CodeLlama-34B-4bit"
    
    if os.path.exists(model_path):
        print(f"å¼€å§‹è®­ç»ƒ: {model_path}")
        trainer.train_large_model(model_path)
    else:
        print(f"âŒ æ¨¡å‹ä¸å­˜åœ¨: {model_path}")
        print("è¯·å…ˆè¿è¡Œé‡åŒ–è„šæœ¬")

if __name__ == "__main__":
    main()
